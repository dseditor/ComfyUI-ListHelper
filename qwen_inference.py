import os
import torch
import folder_paths
import requests
import time
import re
import gc
from typing import Optional, Tuple, Dict

class QwenGPUInference:
    """
    Qwen3-4B GPU æ¨ç†ç¯€é»ï¼ˆå„ªåŒ–è¼‰å…¥é€Ÿåº¦ç‰ˆæœ¬ v3 - æ”¯æ´è¨˜æ†¶é«”ç®¡ç†ï¼‰
    è‡ªå‹•ä¸‹è¼‰æ‰€éœ€é…ç½®æª”æ¡ˆä¸¦ä½¿ç”¨ GPU é€²è¡Œæ¨ç†
    åŒ…å« GPU è¨˜æ†¶é«”æª¢æŸ¥èˆ‡æ¸…ç†åŠŸèƒ½ï¼Œé¿å…èˆ‡ ComfyUI çš„ CLIP æ¨¡å‹è¡çª
    """

    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.current_model_path = None
        self.config_dir = None

    @classmethod
    def _get_safetensors_files(cls):
        """å¾ text_encoders è³‡æ–™å¤¾ä¸­ç²å–æ‰€æœ‰ safetensors æª”æ¡ˆ"""
        safetensors_files = []

        try:
            text_encoder_paths = folder_paths.get_folder_paths("text_encoders")
            for path in text_encoder_paths:
                if os.path.exists(path):
                    for file in os.listdir(path):
                        if file.lower().endswith('.safetensors'):
                            full_path = os.path.join(path, file)
                            if full_path not in safetensors_files:
                                safetensors_files.append(full_path)
        except:
            pass

        if not safetensors_files:
            return ["No safetensors files found"]

        return sorted(safetensors_files)

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "user_prompt": ("STRING", {
                    "multiline": True,
                    "default": "ä¸€å€‹å¥³å­©åœ¨å’–å•¡å»³"
                }),
                "system_prompt": ("STRING", {
                    "multiline": True,
                    "default": """ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ”å½±æç¤ºè©å„ªåŒ–å°ˆå®¶ã€‚ä½ çš„ä»»å‹™æ˜¯å°‡ç°¡å–®çš„å ´æ™¯æè¿°è½‰æ›ç‚ºè©³ç´°ã€å°ˆæ¥­çš„æ”å½±æç¤ºè©ã€‚

è«‹æ ¹æ“šç”¨æˆ¶è¼¸å…¥çš„ç°¡å–®æè¿°ï¼Œç”ŸæˆåŒ…å«ä»¥ä¸‹å…ƒç´ çš„å®Œæ•´æç¤ºè©ï¼š

1. **ä¸»é«”æè¿°**ï¼šè©³ç´°æè¿°ä¸»è¦æ‹æ”å°è±¡ï¼ˆäººç‰©ã€ç‰©é«”ã€å ´æ™¯ï¼‰
2. **ç’°å¢ƒç´°ç¯€**ï¼šå‘¨åœç’°å¢ƒã€èƒŒæ™¯å…ƒç´ ã€å ´æ™¯æ°›åœ
3. **å…‰å½±æ•ˆæœ**ï¼šå…‰ç·šé¡å‹ï¼ˆè‡ªç„¶å…‰/äººé€ å…‰ï¼‰ã€å…‰ç·šæ–¹å‘ã€å…‰å½±å°æ¯”ã€è‰²æº«
4. **ç›¸æ©Ÿè¨­å®š**ï¼šè¦–è§’ã€æ™¯æ·±ã€ç„¦è·æ•ˆæœ
5. **æ§‹åœ–å…ƒç´ **ï¼šç•«é¢ä½ˆå±€ã€å‰æ™¯/ä¸­æ™¯/èƒŒæ™¯é—œä¿‚
6. **è‰²å½©æ°›åœ**ï¼šä¸»è‰²èª¿ã€è‰²å½©æ­é…ã€é£½å’Œåº¦
7. **è³ªæ„Ÿç´°ç¯€**ï¼šæè³ªã€ç´‹ç†ã€ç´°ç¯€è¡¨ç¾
8. **æƒ…ç·’æ°›åœ**ï¼šæ•´é«”æ°›åœã€æƒ…æ„Ÿè¡¨é”

è¼¸å‡ºæ ¼å¼ï¼š
- ä½¿ç”¨è‹±æ–‡è¼¸å‡ºå°ˆæ¥­æ”å½±è¡“èª
- ç”¨é€—è™Ÿåˆ†éš”å„å€‹å…ƒç´ 
- ç¢ºä¿æè¿°å…·é«”ã€å¯è¦–è¦ºåŒ–
- é•·åº¦æ§åˆ¶åœ¨ 150-300 å€‹è‹±æ–‡å–®è©

ç¯„ä¾‹ï¼š
è¼¸å…¥ï¼šä¸€å€‹å¥³å­©åœ¨å’–å•¡å»³
è¼¸å‡ºï¼šA young woman sitting by the window in a cozy coffee shop, warm afternoon sunlight streaming through large glass windows creating soft shadows, wearing casual outfit, holding a cup of coffee, wooden table with laptop and notebook, blurred background with other customers, shallow depth of field, bokeh effect, warm color temperature, golden hour lighting, natural skin tones, professional photography, shot with 50mm lens, f/1.8 aperture, Instagram aesthetic, lifestyle photography, candid moment, peaceful atmosphere"""
                }),
                "max_new_tokens": ("INT", {
                    "default": 2048,
                    "min": 1,
                    "max": 4096,
                    "step": 1
                }),
                "temperature": ("FLOAT", {
                    "default": 0.7,
                    "min": 0.0,
                    "max": 2.0,
                    "step": 0.1
                }),
            },
            "optional": {
                "do_sample": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "æ˜¯å¦ä½¿ç”¨æ¡æ¨£"
                }),
                "top_p": ("FLOAT", {
                    "default": 0.9,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.05
                }),
                "top_k": ("INT", {
                    "default": 50,
                    "min": 0,
                    "max": 100,
                    "step": 1
                }),
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("text",)
    FUNCTION = "inference"
    CATEGORY = "ListHelper"

    def _find_qwen_model(self) -> Optional[str]:
        """è‡ªå‹•å°‹æ‰¾ qwen_3_4b.safetensors æ¨¡å‹"""
        safetensors_files = self._get_safetensors_files()

        # å„ªå…ˆå°‹æ‰¾ qwen_3_4b.safetensors
        for path in safetensors_files:
            if path != "No safetensors files found":
                basename = os.path.basename(path).lower()
                if "qwen" in basename and "3" in basename and "4b" in basename:
                    return path

        # å¦‚æœæ‰¾ä¸åˆ°ç‰¹å®šæ¨¡å‹ï¼Œè¿”å›ç¬¬ä¸€å€‹ safetensors æª”æ¡ˆ
        if safetensors_files and safetensors_files[0] != "No safetensors files found":
            return safetensors_files[0]

        return None

    def _remove_thinking_tags(self, text: str) -> str:
        """ç§»é™¤ <think>...</think> æ¨™ç±¤åŠå…¶å…§å®¹"""
        # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼ç§»é™¤æ‰€æœ‰ <think>...</think> å€å¡Š
        cleaned_text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
        # ç§»é™¤å¤šé¤˜çš„ç©ºç™½è¡Œ
        cleaned_text = re.sub(r'\n\s*\n', '\n', cleaned_text)
        return cleaned_text.strip()

    def _check_gpu_memory(self, required_gb: float = 8.0) -> Tuple[bool, str]:
        """
        æª¢æŸ¥ GPU è¨˜æ†¶é«”æ˜¯å¦è¶³å¤ 

        Args:
            required_gb: éœ€è¦çš„ GPU è¨˜æ†¶é«”å¤§å°ï¼ˆGBï¼‰

        Returns:
            (æ˜¯å¦è¶³å¤ , è©³ç´°è¨Šæ¯)
        """
        if not torch.cuda.is_available():
            return True, "ä½¿ç”¨ CPU æ¨¡å¼ï¼Œç„¡éœ€æª¢æŸ¥ GPU è¨˜æ†¶é«”"

        try:
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            allocated_memory = torch.cuda.memory_allocated(0) / 1024**3
            reserved_memory = torch.cuda.memory_reserved(0) / 1024**3
            free_memory = total_memory - reserved_memory

            info = f"""
GPU è¨˜æ†¶é«”ç‹€æ…‹:
  ç¸½è¨˜æ†¶é«”: {total_memory:.2f} GB
  å·²åˆ†é…: {allocated_memory:.2f} GB
  å·²ä¿ç•™: {reserved_memory:.2f} GB
  å¯ç”¨: {free_memory:.2f} GB
  éœ€è¦: {required_gb:.2f} GB
"""

            if free_memory < required_gb:
                return False, info + f"\nâš ï¸ è¨˜æ†¶é«”ä¸è¶³ï¼ç¼ºå°‘ {required_gb - free_memory:.2f} GB"

            return True, info + "\nâœ“ è¨˜æ†¶é«”å……è¶³"

        except Exception as e:
            return True, f"ç„¡æ³•æª¢æŸ¥ GPU è¨˜æ†¶é«”: {e}"

    def _free_gpu_memory(self) -> None:
        """
        é‡‹æ”¾ GPU è¨˜æ†¶é«”
        æ¸…ç† PyTorch å¿«å–å’ŒåŸ·è¡Œåƒåœ¾å›æ”¶
        """
        try:
            if torch.cuda.is_available():
                # è¨˜éŒ„æ¸…ç†å‰çš„è¨˜æ†¶é«”
                before_allocated = torch.cuda.memory_allocated(0) / 1024**3
                before_reserved = torch.cuda.memory_reserved(0) / 1024**3

                # æ¸…ç† CUDA å¿«å–
                torch.cuda.empty_cache()
                torch.cuda.synchronize()

                # å¼·åˆ¶åƒåœ¾å›æ”¶
                gc.collect()

                # å†æ¬¡æ¸…ç†
                torch.cuda.empty_cache()

                # è¨˜éŒ„æ¸…ç†å¾Œçš„è¨˜æ†¶é«”
                after_allocated = torch.cuda.memory_allocated(0) / 1024**3
                after_reserved = torch.cuda.memory_reserved(0) / 1024**3

                freed_allocated = before_allocated - after_allocated
                freed_reserved = before_reserved - after_reserved

                print(f"\nâœ“ GPU è¨˜æ†¶é«”å·²æ¸…ç†:")
                print(f"  é‡‹æ”¾å·²åˆ†é…è¨˜æ†¶é«”: {freed_allocated:.2f} GB")
                print(f"  é‡‹æ”¾å·²ä¿ç•™è¨˜æ†¶é«”: {freed_reserved:.2f} GB")
                print(f"  ç•¶å‰å·²åˆ†é…: {after_allocated:.2f} GB")
                print(f"  ç•¶å‰å·²ä¿ç•™: {after_reserved:.2f} GB")
            else:
                gc.collect()
                print("âœ“ åŸ·è¡Œåƒåœ¾å›æ”¶ï¼ˆCPU æ¨¡å¼ï¼‰")

        except Exception as e:
            print(f"âš ï¸ æ¸…ç†è¨˜æ†¶é«”æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            # å³ä½¿ç™¼ç”ŸéŒ¯èª¤ï¼Œä»å˜—è©¦åƒåœ¾å›æ”¶
            gc.collect()

    def _download_config_files(self, model_path: str, repo_id: str) -> bool:
        """è‡ªå‹•ä¸‹è¼‰ HuggingFace é…ç½®æª”æ¡ˆ"""
        try:
            model_dir = os.path.dirname(model_path)
            model_basename = os.path.splitext(os.path.basename(model_path))[0]
            self.config_dir = os.path.join(model_dir, f"{model_basename}_config")

            config_files = [
                "config.json",
                "generation_config.json",
                "merges.txt",
                "tokenizer.json",
                "tokenizer_config.json",
                "vocab.json"
            ]

            all_exist = all(os.path.exists(os.path.join(self.config_dir, f)) for f in config_files)

            if all_exist:
                print(f"âœ“ é…ç½®æª”æ¡ˆå·²å­˜åœ¨: {self.config_dir}")
                return True

            os.makedirs(self.config_dir, exist_ok=True)
            print(f"ä¸‹è¼‰é…ç½®æª”æ¡ˆåˆ°: {self.config_dir}")

            base_url = f"https://huggingface.co/{repo_id}/resolve/main/"

            for filename in config_files:
                filepath = os.path.join(self.config_dir, filename)

                if os.path.exists(filepath):
                    print(f"  âœ“ {filename} å·²å­˜åœ¨")
                    continue

                url = base_url + filename
                print(f"  ä¸‹è¼‰ {filename}...")

                try:
                    response = requests.get(url, timeout=30)
                    response.raise_for_status()

                    with open(filepath, 'wb') as f:
                        f.write(response.content)
                    print(f"  âœ“ {filename} ä¸‹è¼‰å®Œæˆ")

                except Exception as e:
                    print(f"  âœ— {filename} ä¸‹è¼‰å¤±æ•—: {str(e)}")
                    continue

            return True

        except Exception as e:
            print(f"âŒ ä¸‹è¼‰é…ç½®æª”æ¡ˆå¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            return False

    def _load_model(self, model_path: str, repo_id: str) -> bool:
        """è¼‰å…¥æ¨¡å‹å’Œ tokenizerï¼ˆå„ªåŒ–ç‰ˆ v3 - åŒ…å«è¨˜æ†¶é«”ç®¡ç†ï¼‰"""
        try:
            if self.model is not None and self.current_model_path == model_path:
                print(f"âœ“ æ¨¡å‹å·²è¼‰å…¥: {os.path.basename(model_path)}")
                return True

            try:
                from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig
                from safetensors.torch import load_file
            except ImportError as e:
                print(f"âŒ ç¼ºå°‘å¿…è¦çš„å¥—ä»¶: {e}")
                print("è«‹åŸ·è¡Œ: pip install transformers safetensors")
                return False

            if not self._download_config_files(model_path, repo_id):
                return False

            print(f"\nè¼‰å…¥æ¨¡å‹: {os.path.basename(model_path)}")
            print("=" * 80)

            # æ­¥é©Ÿ 1: æª¢æŸ¥ GPU è¨˜æ†¶é«”
            print("\næ­¥é©Ÿ 1/3: æª¢æŸ¥ GPU è¨˜æ†¶é«”...")
            is_enough, memory_info = self._check_gpu_memory(required_gb=7.5)
            print(memory_info)

            # æ­¥é©Ÿ 2: å¦‚æœè¨˜æ†¶é«”ä¸è¶³ï¼Œå˜—è©¦æ¸…ç†
            if not is_enough:
                print("\næ­¥é©Ÿ 2/3: è¨˜æ†¶é«”ä¸è¶³ï¼ŒåŸ·è¡Œæ¸…ç†...")
                self._free_gpu_memory()

                # å†æ¬¡æª¢æŸ¥
                is_enough, memory_info = self._check_gpu_memory(required_gb=7.5)
                print("\næ¸…ç†å¾Œçš„è¨˜æ†¶é«”ç‹€æ…‹:")
                print(memory_info)

                if not is_enough:
                    print("\nâš ï¸ GPU è¨˜æ†¶é«”ä»ç„¶ä¸è¶³ï¼Œå°‡ä½¿ç”¨ CPU Offload ç­–ç•¥")
                    print("  - éƒ¨åˆ†æ¨¡å‹å±¤æœƒæ”¾åœ¨ CPUï¼Œæ¨ç†é€Ÿåº¦æœƒè¼ƒæ…¢")
            else:
                print("\næ­¥é©Ÿ 2/3: è¨˜æ†¶é«”å……è¶³ï¼Œè·³éæ¸…ç†")

            print("\næ­¥é©Ÿ 3/3: è¼‰å…¥æ¨¡å‹...")
            overall_start = time.time()

            device = "cuda:0" if torch.cuda.is_available() else "cpu"
            dtype = torch.float16 if torch.cuda.is_available() else torch.float32

            # 1. è¼‰å…¥ tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.config_dir,
                trust_remote_code=True
            )

            # 2. è¼‰å…¥é…ç½®
            config = AutoConfig.from_pretrained(self.config_dir, trust_remote_code=True)

            # 3. æ ¹æ“šè¨˜æ†¶é«”æƒ…æ³é¸æ“‡è¼‰å…¥ç­–ç•¥
            import shutil
            temp_model_dir = os.path.join(self.config_dir, "temp_model")
            os.makedirs(temp_model_dir, exist_ok=True)

            # æ±ºå®šè¼‰å…¥ç­–ç•¥
            if torch.cuda.is_available():
                free_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3 - torch.cuda.memory_reserved(0) / 1024**3

                if free_memory >= 7.5:
                    # å……è¶³è¨˜æ†¶é«”ï¼šå®Œå…¨è¼‰å…¥åˆ° GPU
                    print(f"âš¡ ç­–ç•¥ 1: å®Œå…¨ GPU è¼‰å…¥ï¼ˆå¯ç”¨è¨˜æ†¶é«”: {free_memory:.2f} GBï¼‰")
                    max_memory_config = None
                    offload_folder = None
                else:
                    # è¨˜æ†¶é«”ä¸è¶³ï¼šä½¿ç”¨ CPU Offload
                    available_gpu = max(3.0, free_memory - 1.0)  # è‡³å°‘ä¿ç•™ 1GB çµ¦å…¶ä»–æ“ä½œ
                    print(f"âš¡ ç­–ç•¥ 2: CPU Offloadï¼ˆå¯ç”¨ GPU: {free_memory:.2f} GBï¼Œåˆ†é…: {available_gpu:.2f} GBï¼‰")
                    print(f"  - éƒ¨åˆ†æ¨¡å‹å±¤å°‡æ”¾åœ¨ CPUï¼Œæ¨ç†é€Ÿåº¦æœƒè¼ƒæ…¢")
                    max_memory_config = {
                        0: f"{available_gpu:.1f}GB",
                        "cpu": "16GB"
                    }
                    # å‰µå»º offload è³‡æ–™å¤¾
                    offload_folder = os.path.join(self.config_dir, "offload")
                    os.makedirs(offload_folder, exist_ok=True)
            else:
                print("âš¡ ç­–ç•¥ 3: CPU æ¨¡å¼")
                max_memory_config = None
                offload_folder = None

            load_start = time.time()

            try:
                # è¤‡è£½é…ç½®æª”æ¡ˆ
                for file in ["config.json", "generation_config.json"]:
                    src = os.path.join(self.config_dir, file)
                    if os.path.exists(src):
                        shutil.copy(src, temp_model_dir)

                # å‰µå»ºç¬¦è™Ÿé€£çµæˆ–è¤‡è£½ safetensors
                safetensors_target = os.path.join(temp_model_dir, "model.safetensors")
                if os.path.exists(safetensors_target):
                    os.remove(safetensors_target)

                # Windows ä½¿ç”¨ç¡¬é€£çµè€Œä¸æ˜¯ç¬¦è™Ÿé€£çµ
                try:
                    os.link(model_path, safetensors_target)
                except:
                    shutil.copy(model_path, safetensors_target)

                # ä½¿ç”¨é©ç•¶çš„è¼‰å…¥ç­–ç•¥
                load_kwargs = {
                    "pretrained_model_name_or_path": temp_model_dir,
                    "trust_remote_code": True,
                    "device_map": "auto",
                    "torch_dtype": dtype,
                    "low_cpu_mem_usage": True
                }

                if max_memory_config is not None:
                    load_kwargs["max_memory"] = max_memory_config

                if offload_folder is not None:
                    load_kwargs["offload_folder"] = offload_folder

                self.model = AutoModelForCausalLM.from_pretrained(**load_kwargs)

                print(f"  âœ“ æ¨¡å‹è¼‰å…¥å®Œæˆï¼ˆè€—æ™‚: {time.time() - load_start:.2f} ç§’ï¼‰")

            finally:
                # æ¸…ç†è‡¨æ™‚ç›®éŒ„
                try:
                    if os.path.exists(temp_model_dir):
                        shutil.rmtree(temp_model_dir)
                except:
                    pass

            missing_keys = []
            unexpected_keys = []

            if missing_keys:
                print(f"  è­¦å‘Š: ç¼ºå°‘çš„éµå€¼: {len(missing_keys)} å€‹")
            if unexpected_keys:
                print(f"  è­¦å‘Š: æœªé æœŸçš„éµå€¼: {len(unexpected_keys)} å€‹")

            self.current_model_path = model_path
            total_time = time.time() - overall_start

            print(f"\né©—è­‰æ¨¡å‹ç‹€æ…‹...")
            if torch.cuda.is_available():
                current_allocated = torch.cuda.memory_allocated(0) / 1024**3
                current_reserved = torch.cuda.memory_reserved(0) / 1024**3
                print(f"  âœ“ æ¨¡å‹å·²è¼‰å…¥")
                print(f"  - GPU è¨˜æ†¶é«”å·²åˆ†é…: {current_allocated:.2f} GB")
                print(f"  - GPU è¨˜æ†¶é«”å·²ä¿ç•™: {current_reserved:.2f} GB")

                # æª¢æŸ¥æ¨¡å‹è¨­å‚™åˆ†ä½ˆ
                device_map = {}
                for name, param in self.model.named_parameters():
                    device_str = str(param.device)
                    device_map[device_str] = device_map.get(device_str, 0) + 1

                print(f"  - æ¨¡å‹è¨­å‚™åˆ†ä½ˆ:")
                for device_name, count in device_map.items():
                    print(f"    * {device_name}: {count} å€‹åƒæ•¸")
            else:
                print(f"  âœ“ æ¨¡å‹åœ¨ CPU ä¸Š")

            print("\n" + "=" * 80)
            print(f"âœ“ æ¨¡å‹è¼‰å…¥æˆåŠŸï¼ˆç¸½è€—æ™‚: {total_time:.2f} ç§’ï¼‰")
            print("=" * 80)

            # çµ¦å‡ºå„ªåŒ–å»ºè­°
            if total_time > 60:
                print(f"\nğŸ’¡ è¼‰å…¥å„ªåŒ–å»ºè­°:")
                print(f"  - ç•¶å‰è¼‰å…¥æ™‚é–“: {total_time:.1f} ç§’")
                print(f"  - ä¸»è¦ç“¶é ¸: ç§»å‹•æ¨¡å‹åˆ° GPU")
                print(f"  - é€™æ˜¯æ­£å¸¸çš„ï¼Œç„¡æ³•é€²ä¸€æ­¥å„ªåŒ–ï¼ˆç¡¬é«”é™åˆ¶ï¼‰")
                print(f"  - æ¨¡å‹æœƒä¿ç•™åœ¨è¨˜æ†¶é«”ä¸­ï¼Œä¸‹æ¬¡ä½¿ç”¨æœƒå³æ™‚è¼‰å…¥")

            print()
            return True

        except Exception as e:
            print(f"âŒ è¼‰å…¥æ¨¡å‹å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            self.model = None
            self.tokenizer = None
            self.current_model_path = None
            return False

    def inference(
        self,
        user_prompt: str,
        system_prompt: str,
        max_new_tokens: int,
        temperature: float,
        do_sample: bool = True,
        top_p: float = 0.9,
        top_k: int = 50,
    ) -> Tuple[str]:
        """åŸ·è¡Œæ¨ç†"""

        # è‡ªå‹•å°‹æ‰¾ Qwen æ¨¡å‹
        model_path = self._find_qwen_model()
        if model_path is None:
            return ("âŒ éŒ¯èª¤: æœªæ‰¾åˆ° Qwen æ¨¡å‹æª”æ¡ˆ\nè«‹å°‡ qwen_3_4b.safetensors æª”æ¡ˆæ”¾åœ¨ ComfyUI çš„ models/text_encoders è³‡æ–™å¤¾ä¸­",)

        if not os.path.exists(model_path):
            return (f"âŒ éŒ¯èª¤: æ¨¡å‹æª”æ¡ˆä¸å­˜åœ¨: {model_path}",)

        # ä½¿ç”¨å›ºå®šçš„ repo_id
        repo_id = "Qwen/Qwen3-4B"
        if not self._load_model(model_path, repo_id):
            return ("âŒ éŒ¯èª¤: æ¨¡å‹è¼‰å…¥å¤±æ•—",)

        try:
            messages = []
            if system_prompt and system_prompt.strip():
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": user_prompt})

            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )

            inputs = self.tokenizer(text, return_tensors="pt")

            device = next(self.model.parameters()).device
            inputs = {k: v.to(device) for k, v in inputs.items()}

            inference_start = time.time()
            print(f"é–‹å§‹æ¨ç†...")

            with torch.no_grad():
                outputs = self.model.generate(
                    input_ids=inputs['input_ids'],
                    attention_mask=inputs['attention_mask'],
                    max_new_tokens=max_new_tokens,
                    do_sample=do_sample,
                    temperature=temperature,
                    top_p=top_p,
                    top_k=top_k,
                    pad_token_id=self.tokenizer.eos_token_id
                )

            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

            # ç§»é™¤ assistant æ¨™è¨˜
            if "assistant" in response:
                for separator in ["<|im_start|>assistant\n", "assistant\n", "Assistant:", "assistant:"]:
                    if separator in response:
                        response = response.split(separator)[-1].strip()
                        break

            # ç§»é™¤ <think> æ¨™ç±¤
            response = self._remove_thinking_tags(response)

            inference_time = time.time() - inference_start

            tokens_generated = len(outputs[0]) - len(inputs['input_ids'][0])
            tokens_per_sec = tokens_generated / inference_time if inference_time > 0 else 0

            print(f"âœ“ æ¨ç†å®Œæˆï¼ˆè€—æ™‚: {inference_time:.2f} ç§’ï¼‰")
            print(f"  ç”Ÿæˆ tokens: {tokens_generated}")
            print(f"  é€Ÿåº¦: {tokens_per_sec:.1f} tokens/ç§’")

            if torch.cuda.is_available():
                print(f"  GPU è¨˜æ†¶é«”ä½¿ç”¨: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB")
                print(f"  GPU è¨˜æ†¶é«”å³°å€¼: {torch.cuda.max_memory_allocated(0) / 1024**3:.2f} GB")

            return (response,)

        except Exception as e:
            import traceback
            error_msg = f"âŒ æ¨ç†å¤±æ•—: {str(e)}\n{traceback.format_exc()}"
            print(error_msg)
            return (error_msg,)
